<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>VAEriants</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <script type="text/javascript" 
    src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
	MathJax.Hub.Config({
	  tex2jax: {
		displayMath: [ ['$$','$$'], ['\[','\]'] ],
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true
	  }
	});

  </script>
</head>

  <body>
  <section class="post-header">
	<img src="/assets/mila_logo_fr.svg">
	<span class="project-name">VAEriants</span>
</section>
<div style="display:none">

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\h}{h}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ELBO}{\mathcal{L}}
\newcommand{\prob}[3]{{#1}_{#2} \left( #3 \right)}
\newcommand{\condprob}[4]{{#1}_{#2} \left( #3 \middle| #4 \right)}
\newcommand{\expected}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\KL}[2]{D_{\mathrm{KL}}\left( #1 \| #2 \right)}
\newcommand{\pp}{\theta}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\layers}{L}
\newcommand{\dimpp}{D}
$$

</div>


    <section class="main-content">
      <h1 id="variational-inference">Variational Inference</h1>

<h2 id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h2>
<p>Consider the following setting: We have an $\x$ we observe, and the generative process for this $\x$ is to sample $\z$ from a simple distribution that we know ($\prob{p}{}{\z}$) and some mysterious stochastic process turns $\z$ into $\x$  that we also know ($\condprob{p}{}{\x}{\z}$). So since we know these two pieces, figuring out how likely any given $\x$ is requires us to do this:</p>

<div class="kdmath">$$
\prob{p}{}{\x} = \int \condprob{p}{}{\x}{\z} \prob{p}{}{\z} \mathrm{d}\z
$$</div>

<p>Ooh. Big scary integral. And we have to maximise over it. What do?</p>

<p>Unless our $\condprob{p}{}{\x}{\z}$ is simple (a linear mapping, for example), this is intractable to compute. One way around this, is to think of it as an expectation over $\z$:</p>

<div class="kdmath">$$
\prob{p}{}{\x} = \expected{\prob{p}{}{\z}}{\condprob{p}{}{\x}{\z}}
$$</div>

<p>then we can estimate this quantity by sampling from $\prob{p}{}{\z}$ multiple times, and then averaging over the output,</p>

<div class="kdmath">$$
\approx \frac{1}{N} \sum^N_{i=1} \condprob{p}{}{\x}{\z_i}
$$</div>

<!--<table class="image">
<tr><td><img src="/figures/mixture_bad.png" alt="When sampling from the prior (say, it's a uniform binary code), we end up updating the same class-conditional ${\condprob{p}{}{\x=x}{\z=j}}$ for some $j\in\{1,2\}$ and for all $x$ in the data set. 
Thus, there's no difference between the learned latent code $j$ (top).
If we sample more cleverly by grouping the data points, and assigning larger probability to $p_1(z=j)$ for group 1 and to $p_2$ for group 2, different latent code $j$ will be updated to correspond to a different and sharper class-conditional likelihood ${\condprob{p}{}{\x}{\z=j}}$ (bottom)."/></td></tr>
<caption align="bottom">When sampling from the prior (say, it's a uniform binary code), we end up updating the same class-conditional ${\condprob{p}{}{\x=x}{\z=j}}$ for some $j\in\{1,2\}$ and for all $x$ in the data set. 
Thus, there's no difference between the learned latent code $j$ (top).
If we sample more cleverly by grouping the data points, and assigning larger probability to $p_1(z=j)$ for group 1 and to $p_2$ for group 2, different latent code $j$ will be updated to correspond to a different and sharper class-conditional likelihood ${\condprob{p}{}{\x}{\z=j}}$ (bottom).</caption>
</table>
 -->

<table class="image">
<tr><td><img src="/figures/q_vs_noq.png" alt="Left: Sampling from $\prob{p}{}{\x}$, Right: Sampling from $\condprob{q}{}{\x}{\z}$. Using an approximate posterior $q$ narrows down the region in the latent space." /></td></tr>
<caption align="bottom">Left: Sampling from $\prob{p}{}{\x}$, Right: Sampling from $\condprob{q}{}{\x}{\z}$. Using an approximate posterior $q$ narrows down the region in the latent space.</caption>
</table>

<p>The problem with this approach is that it has <em>high variance</em> (see Figure \ref{fig:fromprior1} and \ref{fig:fromprior2}). Think about what we are trying to do here: We have $\x$, and we’re trying to maximise $\prob{p}{}{\z}$. However, what the latent variable model is saying is that there is \emph{some} or \emph{several} $\z$s that will generate the observed $\x$. But because we don’t know what those $\z$s are, we’re just randomly picking out a few from the distribution, pushing them through the generative process, and hoping they hit the mark.</p>

<p>Here’s where $q$ comes in to save the day (Yes, just like Star Trek).</p>

<p>We come up with an <em>approximate posterior</em>, or in deep learning parlance, an encoder, $\condprob{q}{}{\z}{\x}$. Its mission? To -boldly go where- narrow down the area in the prior distribution $\prob{p}{}{\z}$ so that we have an easier time generating something close to the true $\z$ that corresponds to the observed $\x$.</p>

<p>Now, to relate it back to the information-theoretic point-of-view, narrowing down the search space is akin to giving the model more ``information’’ about where in the latent space the corresponding latent state is.</p>

<p>Of course, none of this comes for free. Let’s take a look at what introducing $q$ means:</p>

<div class="kdmath">$$
\begin{align*}
\prob{p}{}{\x} 
&= \int \condprob{p}{}{\x}{\z} \prob{p}{}{\z} \mathrm{d}\z \\
&= \int \underbrace{\condprob{q}{}{\z}{\x} \frac{\condprob{p}{}{\x}{\z} \prob{p}{}{\z}}{\condprob{q}{}{\z}{\x}}}_{\text{multiplication by 1!}} \mathrm{d}\z = \expected{\condprob{q}{}{\z}{\x}}{\condprob{p}{}{\x}{\z} 
\frac{\prob{p}{}{\z}}{\condprob{q}{}{\z}{\x}}} 
\end{align*}
$$</div>

<p>Again, we can sample, but this time from $\condprob{q}{}{\z}{\x}$. 
However, notice that the quantity is now weighted by the ratio $\frac{\prob{p}{}{\z}}{\condprob{q}{}{\z}{\x}}$.</p>

<p>So, if we’re trying to maximise $\log \prob{p}{}{\x}$,</p>

<div class="kdmath">$$
\begin{align*}
\log \prob{p}{}{\x} &= 
\log \expected{\condprob{q}{}{\z}{\x}}{
\condprob{p}{}{\x}{\z} \frac{\prob{p}{}{\z}}{\condprob{q}{}{\z}{\x}}} \\
&\geq \expected{\condprob{q}{}{\z}{\x}}{\log
\condprob{p}{\theta}{\x}{\z} \frac{\prob{p}{\theta}{\z}}{\condprob{q}{\phi}{\z}{\x}}} = \ELBO(\theta, \phi;x) 
\end{align*}
$$</div>

<p>and we get the lower-bound. We use subscripts to denote parameters of the distributions.</p>

<p>The gap between $\log \prob{p}{}{\x}$ and this lower-bound is known as the \emph{variational gap}, and has its own interpretation (Covered in some appendix).</p>

      <h2> What's next? </h2>
      <ul>
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
            <li><a href="/pages/vae.html">The Variational Autoencoder</a></li>
        
      
        
        
      
        
        
      
      </ul>
      <h2> References </h2>
      <ol class="bibliography"></ol>
      <footer class="site-footer">
  <span class="site-footer-credits"><a href="https://mila.umontreal.ca">MILA: Montreal Institute for Learning Algorithms</a></span>
  <span class="site-footer-owner"><a href="http://localhost:4000">VAEriants</a> is maintained by <a href="https://blog.wtf.sg">Shawn Tan</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/pietromenna/jekyll-cayman-theme">Cayman theme</a> by <a href="http://github.com/jasonlong">Jason Long</a>.</span>
</footer>

    </section>

  </body>
</html>
