<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>VAEriants</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <script type="text/javascript" 
    src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
	MathJax.Hub.Config({
	  tex2jax: {
		displayMath: [ ['$$','$$'], ['\[','\]'] ],
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true
	  }
	});

  </script>
</head>

  <body>
  <section class="post-header">
	<img src="/assets/mila_logo_fr.svg">
	<a href="/" class="project-name">VAEriants</a>
</section>
<div style="display:none">

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\h}{h}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ELBO}{\mathcal{L}}
\newcommand{\prob}[3]{{#1}_{#2} \left( #3 \right)}
\newcommand{\condprob}[4]{{#1}_{#2} \left( #3 \middle| #4 \right)}
\newcommand{\expected}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\KL}[2]{D_{\mathrm{KL}}\left( #1 \| #2 \right)}
\newcommand{\pp}{\theta}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\layers}{L}
\newcommand{\dimpp}{D}
$$

</div>


    <section class="main-content">
      <h1 id="the-holy-trinity">The Holy Trinity</h1>

<p>If we consider the standard, single-latent-variable model, there are three main pieces we define, and then optimise over:</p>

<ul>
  <li>
    <p><strong>The approximate posterior</strong></p>

    <div class="kdmath">$$
\condprob{q}{}{\z}{\x}
$$</div>

    <p><em>Otherwise called the _encoder</em>, or the _inference network in the context of VAEs, where they’re parameterised by a deep network (written here as $f$).
The conditional distribution over $\z$ given the data $\x$ is usually defined as Gaussian, with parameters that are a function of $\x$,  <span class="kdmath">$\condprob{q}{}{\z}{\x} = \N\left(f_\mu(\x), f_\sigma(\x)\right)$</span>.</p>
  </li>
  <li>
    <p><strong>The likelihood</strong></p>

    <div class="kdmath">$$
\condprob{p}{}{\x}{\z}
$$</div>

    <p>Otherwise called the <em>decoder</em>, or the <em>generative</em> network in the context of VAEs, where they’re also parameterised by a deep network.</p>
  </li>
  <li>
    <p><strong>The prior</strong></p>

    <div class="kdmath">$$
\prob{p}{}{\z}
$$</div>

    <p>In <a href="#kingma2013auto">(Kingma &amp; Welling, 2013)</a>, this is defined $\N(\mathbf{0}, \mathbf{1})$  and not parameterised.</p>
  </li>
</ul>

<p>All three aspects of the model can be improved, not only at the level of architecture, but in the types of distributions that are assumed for each of them in the model.</p>


      <h2> What's next? </h2>
      <ul>
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
            <li><a href="/pages/improving_likelihood.html">Improving the Likelihood</a></li>
        
      
        
        
            <li><a href="/pages/improving_pos.html">Improving the Posterior</a></li>
        
      
        
        
            <li><a href="/pages/improving_prior.html">Improving the Prior</a></li>
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
            <li><a href="/pages/variance_reduction.html">Variance Reduction</a></li>
        
      
      </ul>
      <h2> References </h2>
      <ol class="bibliography"><li><span id="kingma2013auto">Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. <i>ArXiv Preprint ArXiv:1312.6114</i>.</span></li></ol>
      <footer class="site-footer">
  <span class="site-footer-credits"><a href="https://mila.umontreal.ca">MILA: Montreal Institute for Learning Algorithms</a></span>
  <span class="site-footer-owner"><a href="">VAEriants</a> is maintained by <a href="https://blog.wtf.sg">Shawn Tan</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/pietromenna/jekyll-cayman-theme">Cayman theme</a> by <a href="http://github.com/jasonlong">Jason Long</a>.</span>
</footer>

    </section>

  </body>
</html>
