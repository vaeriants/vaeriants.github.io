<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>VAEriants</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <script type="text/javascript" 
    src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
	MathJax.Hub.Config({
	  tex2jax: {
		displayMath: [ ['$$','$$'], ['\[','\]'] ],
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true
	  }
	});

  </script>
</head>

  <body>
  <section class="post-header">
	<img src="/assets/mila_logo_fr.svg">
	<a href="/" class="project-name">VAEriants</a>
</section>
<div style="display:none">

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\h}{h}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ELBO}{\mathcal{L}}
\newcommand{\prob}[3]{{#1}_{#2} \left( #3 \right)}
\newcommand{\condprob}[4]{{#1}_{#2} \left( #3 \middle| #4 \right)}
\newcommand{\expected}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\KL}[2]{D_{\mathrm{KL}}\left( #1 \| #2 \right)}
\newcommand{\pp}{\theta}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\layers}{L}
\newcommand{\dimpp}{D}
$$

</div>


    <section class="main-content">
      <h1 id="improving-the-likelihood">Improving the Likelihood</h1>

<p>Consider modelling an image using a single latent variable model.
In order to produce a reasonable, realistic looking image, we need to decide the objects in the image, their location, abstract stuff.
But beyond these abstract ideas, there are also the details, like lighting and texture that we need to model.</p>

<p>When the probability of the given datapoint is defined as being factorised over every dimension, the implication is that the distribution over the observation is assumed to be completely dependent upon the latent variable.
However, the latent variable has limited capacity, and may model only the aspects that contribute to most of the reconstruction loss (abstract concepts) and there may be variations at a lower level of abstraction that may depend on other parts of the input (details).</p>

<p>Modelling the data this way are what we’ll call autoregressive generative models, and some examples of such models are <a href="#gulrajani2016pixelvae">(Gulrajani et al., 2016)</a> for images, with a slightly more general discussion found in <a href="#chen2016variational">(Chen et al., 2016)</a>, and <a href="#bowman2015generating">(Bowman et al., 2015)</a> for text.</p>


      <h2> What's next? </h2>
      <ul>
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
            <li><a href="/pages/improving_pos.html">Improving the Posterior</a></li>
        
      
        
        
            <li><a href="/pages/improving_prior.html">Improving the Prior</a></li>
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
      </ul>
      <h2> References </h2>
      <ol class="bibliography"><li><span id="gulrajani2016pixelvae">Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., &amp; Courville, A. (2016). Pixelvae: A latent variable model for natural images. <i>ArXiv Preprint ArXiv:1611.05013</i>.</span></li>
<li><span id="chen2016variational">Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., … Abbeel, P. (2016). Variational lossy autoencoder. <i>ArXiv Preprint ArXiv:1611.02731</i>.</span></li>
<li><span id="bowman2015generating">Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp; Bengio, S. (2015). Generating sentences from a continuous space. <i>ArXiv Preprint ArXiv:1511.06349</i>.</span></li></ol>
      <footer class="site-footer">
  <span class="site-footer-credits"><a href="https://mila.umontreal.ca">MILA: Montreal Institute for Learning Algorithms</a></span>
  <span class="site-footer-owner"><a href="">VAEriants</a> is maintained by <a href="https://blog.wtf.sg">Shawn Tan</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/pietromenna/jekyll-cayman-theme">Cayman theme</a> by <a href="http://github.com/jasonlong">Jason Long</a>.</span>
</footer>

    </section>

  </body>
</html>
