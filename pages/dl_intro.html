<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>VAEriants</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <script type="text/javascript" 
    src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
	MathJax.Hub.Config({
	  tex2jax: {
		displayMath: [ ['$$','$$'], ['\[','\]'] ],
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true
	  }
	});

  </script>
</head>

  <body>
  <section class="post-header">
	<img src="/assets/mila_logo_fr.svg">
	<a href="/" class="project-name">VAEriants</a>
</section>
<div style="display:none">

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\h}{h}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ELBO}{\mathcal{L}}
\newcommand{\prob}[3]{{#1}_{#2} \left( #3 \right)}
\newcommand{\condprob}[4]{{#1}_{#2} \left( #3 \middle| #4 \right)}
\newcommand{\expected}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\KL}[2]{D_{\mathrm{KL}}\left( #1 \| #2 \right)}
\newcommand{\pp}{\theta}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\layers}{L}
\newcommand{\dimpp}{D}
$$

</div>


    <section class="main-content">
      <h1 id="introduction">Introduction</h1>

<p>In the long history of research into artificial neural networks, autoencoders are an interesting topic.
The autoencoder is trained to be an identity</p>

<p>They are commonly used to learn an encoding of the input. Most use a dimension bottleneck in the hidden layers to achieve a non-linear dimension reduction, but it is also sometimes useful to use a higher dimension, like in sparse autoencoders.
During the resurgence of neural networks as deep learning, autoencoders were largely used to do pretraining.
(missing reference) first interprets a special kind of autoencoder, called the denoising autoencoders, as a generative model.
One connection to make is that in their work, an unstructured noise is injected to the data,</p>

<p>The introduction of VAEs <a href="#kingma2013auto">(Kingma &amp; Welling, 2013)</a> interprets the autoencoder as a graphical model with the encoding being a latent variable to be inferred.
While autoencoders on their own weren’t a probabilistic model before, the VAE framework offers an alternative look at them as such.</p>

<p>Because they were introduced as a generative model, one of the more common applications of VAEs is to generate images, i.e: sampling from a Gaussian distribution and pushing that through the decoder.
Another application of VAEs are what autoencoders were used for before: learning non-linear representations of the input.</p>

<p>However, the framework VAEs introduce is capable of much more. 
In DeepMind’s paper on the topic, they refer to these models as Deep Latent Gaussian Models (DLGMs) <a href="#rezende2014stochastic">(Rezende, Mohamed, &amp; Wierstra, 2014)</a>.
We think this may be more descriptive, although we need not limit ourselves to Gaussian distributions.
Sure, you <em>can</em> use VAEs to autoencode, but using them for just that is missing the point <a href="#chen2016variational">(Chen et al., 2016)</a>.</p>


      <h2> What's next? </h2>
      <ul>
      
        
        
            <li><a href="/pages/background.html">Background on Variational Inference</a></li>
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
            <li><a href="/pages/prob_intro.html">More familiar with Probabilistic Graphical Models?</a></li>
        
      
        
        
      
        
        
      
        
        
      
      </ul>
      <h2> References </h2>
      <ol class="bibliography"><li><span id="kingma2013auto">Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. <i>ArXiv Preprint ArXiv:1312.6114</i>.</span></li>
<li><span id="rezende2014stochastic">Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. <i>ArXiv Preprint ArXiv:1401.4082</i>.</span></li>
<li><span id="chen2016variational">Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., … Abbeel, P. (2016). Variational lossy autoencoder. <i>ArXiv Preprint ArXiv:1611.02731</i>.</span></li></ol>
      <footer class="site-footer">
  <span class="site-footer-credits"><a href="https://mila.umontreal.ca">MILA: Montreal Institute for Learning Algorithms</a></span>
  <span class="site-footer-owner"><a href="">VAEriants</a> is maintained by <a href="https://blog.wtf.sg">Shawn Tan</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/pietromenna/jekyll-cayman-theme">Cayman theme</a> by <a href="http://github.com/jasonlong">Jason Long</a>.</span>
</footer>

    </section>

  </body>
</html>
